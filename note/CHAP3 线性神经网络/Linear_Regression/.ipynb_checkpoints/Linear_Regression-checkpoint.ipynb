{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **回归**是为一个或多个自变量和因变量之间关系建模的一种方法。机器学习中大多数任务与预测相关，常涉及回归问题（但是不全是回归问题，还有分类问题）\n",
    "\n",
    "回归问题的术语：\n",
    "1. training (data) set: 一个真实的数据集，用于训练模型\n",
    "2. sample/data point/data instance: 训练集中的一行数据\n",
    "3. label/target: 想要预测的目标\n",
    "4. feature/covariate: 预测所依据的自变量/协变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定d个输入数据$X=(x_1, x_2, \\ldots, x_d)$，欲求得预测结果$\\hat{y}$。利用线性回归模型，假设\n",
    "$$\n",
    "\\hat{y} = w_1  x_1 + ... + w_d  x_d + b = \\mathbf{w}^\\top \\mathbf{x} + b.\n",
    "$$\n",
    "其中$w_k$为权重，$b$为偏移量(bias/offset/intercept)。实际上是进行了一个**仿射变换**(通过加权和进行线性变换，通过偏移量进行平移)。\n",
    "\n",
    "对于一组数据集，可以使用\n",
    "$$\n",
    "{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b\n",
    "$$\n",
    "来表示预测值向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用损失函数量化实际值和预测值之间的差距，以作为对拟合程度的度量（通常选择非负数，越小代表损失越少）\n",
    "\n",
    "通常选择平方误差函数：\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n",
    "$$\n",
    "\n",
    "对整个数据集：\n",
    "$$\n",
    "L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n",
    "$$\n",
    "\n",
    "则希望找一组参数$\\mathbf{w}^*, b^*$，使得能最小化在所有数据集上的损失：\n",
    "$$\n",
    "\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 解析解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型的最优参数能够用解析解表示（其它大多数模型不能）：\n",
    "$$\n",
    "\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}. \\\\\n",
    "b^* = \\mathbf{y} - \\mathbf{X} \\mathbf{w}^*\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用梯度下降法可以优化几乎所有深度学习模型。最简单的方法：计算全数据集的损失函数关于参数向量的梯度，并不断在损失函数递减的方向上更新参数以降低误差。但是这样需要遍历整个数据集，效率低下。可以使用**小批量随机梯度下降**（minibatch stochastic gradient descent）：\n",
    "\n",
    "> 在每次迭代中，我们首先随机抽样一个小批量$\\mathcal{B}$，它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数$\\eta$，并从当前参数的值中减掉。\n",
    "\n",
    "即\n",
    "$$\n",
    "(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).\n",
    "$$\n",
    "或\n",
    "$$\n",
    "\\begin{split}\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}\\end{split}\n",
    "$$\n",
    "其中，$\\mathcal{B}$称为批量大小（batch size）, $\\eta$称为学习率(learning rate)，这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）\n",
    "\n",
    "在训练到一定轮数或达到一定要求后，记录下参数的估计值$\\hat{\\mathbf{w}}, \\hat{b}$。无论如何都无法在有限步中通过梯度下降法求得真正的最优参数。更难的是，找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失，这被称为泛化（generalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、矢量加速"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于Python中的for循环开销大，而矢量运算开销小，故应该多将数据**矢量化**，再调用线性代数库，以实现加速的目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个计时器以测试耗时："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Timer:  #@save\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正态分布满足公式：\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).\n",
    "$$\n",
    "可定义函数以计算正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以调用`torch.normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.normal(0, 1, (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进行可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次使用numpy进行可视化\n",
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "# 均值和标准差对\n",
    "params = [(0, 1), (0, 2), (3, 1)]\n",
    "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n",
    "         ylabel='p(x)', figsize=(4.5, 2.5),\n",
    "         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均方误差损失函数可用于线性回归的一个原因：我们假设了观测中包含噪声\n",
    "$$\n",
    "y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon,\n",
    "$$\n",
    "其中噪声服从正态分布\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "从这个假设可以逐步推导：\n",
    "1. 通过**给定的**$\\mathbf{x}$观测到**特定的**$y$的**似然**\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).\n",
    "$$\n",
    "2. 根据最大似然估计法，最优参数是使整个数据集的似然最大的值\n",
    "$$\n",
    "P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).\n",
    "$$\n",
    "3. 可以使用负对数进行简化\n",
    "$$\n",
    "-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n (\\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2).\n",
    "$$\n",
    "4. 去掉不变项，得到最简形式\n",
    "$$\n",
    "\\sum_{i=1}^n \\frac{1}{2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2\n",
    "$$\n",
    "即为均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、线性回归和神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归可以被看作是一个单层神经网络，如：\n",
    "\n",
    "![](https://zh-v2.d2l.ai/_images/singleneuron.svg)\n",
    "\n",
    "这一个单层被称为**全连接层**(fully-connected layer)，在PyTorch中全连接层在`Linear`类中定义(需要接受两个参数，第一个为输入特征形状，第二个为输出特征形状)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
