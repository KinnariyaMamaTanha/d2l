{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Pretraining With Transformers\n",
    "\n",
    "## Encoder-Only\n",
    "\n",
    "- 只能生成和 input 相同长度的表示，不过可以进一步投影到需要的维度的 output。\n",
    "- 代表模型：BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "### Pretraining BERT\n",
    "\n",
    "利用 input 和 representation 的长度一致，可以通过 **mask** 的方式进行预训练。例如，在句子 `I love this red car` 中，掩盖掉 `love`，代替为一个特殊的 token `<mask>`，也就是输入 `I <mask> this red car`，希望得到 `I love this red car`。\n",
    "\n",
    "另外，和训练 transformer 时不同，输入的句子中每个 token 都可以注意到*其它所有* token。\n",
    "\n",
    "计算 loss 时，分别计算被掩盖和未被掩盖的 token 的交叉熵损失，最后进行加权和。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
